# 활성화함수(activation function)

- 딥러닝 네트워크에서는 노드에 들어오는 값들을 곧바로 다음 레이어로 전달하지 않고 주로 비선형 함수를 통과시켜 전달, 이 때 사용하는 함수가 `활성화 함수`

- neural network에서 층을 쌓을 때는 활성화 함수로 비선형 함수를 사용해야 함

---



---

### Sigmoid Function (시그모이드 함수)

- `0~1 `사이의 값만 가지는 비선형 함수
- 보통 시그모이드 함수는 이진 분류(0,1) 출력을 가지는 출력층에서만 사용됨(은닉층에서는 거의 사용X)

---



---

### Tanh Function (하이퍼볼릭탄젠트 함수)

- `-1~1` 사이의 값만 가지는 비선형 함수
- 시그모이드 함수를 평행이동한 것과 수학적으로 동일
- 데이터의 중심을 0으로 위치시키는 효과가 있어 다음 층의 학습이 더 쉽게 이루어짐
- 따라서 보통 시그모이드 함수보다 성능이 좋음

---



---

### 위 두 함수(Sigmoid, Tanh)의 단점

- x가 매우 크거나 매우 작을 때, 함수의 gradient(미분값,기울기)가 거의 0이 됨

  ---> `Gradient Vanishing` : layer가 늘어날 때 값이 사라지는 현상, 기울기 소실 문제

- 이를 해결하기 위한 함수가 `Relu Function (렐루 함수)`

---



---

### Relu Function (렐루 함수)

- x가 양수이면 미분값은 1, x가 음수이면 미분값은 0
- Gradient Vanishing 문제가 해결되어 가장 기본적인 활성화 함수로 사용 됨
- 단점은 x가 음수일 때 미분값이 0이라는 점이지만 실제로는 잘 작동함
- 거의 모든 CNN network나 딥러닝에 사용됨

---



---

### Leaky Relu Function (리키 렐루 함수)

- x가 음수일 때 미분값이 0이 되는(Dying Relu) 렐루 함수 대신, 약간의 기울기를 갖는 함수
- 대개 렐루 함수보다 잘 작동하지만 실제로는 거의 사용하지 않음

---



---

### Softmax Function (소프트맥스 함수)

- 다중 클래스 분류 모델 생성 시 사용
- 출력층에서 사용
- input 값을 0~1 사이의 값으로 모두 정규화하여 출력, 출력값들의 총합은 항상 `1`

- 확률값을 반환한다는 점에서 시그모이드와 비슷하지만, 시그모이드 함수를 통과해 얻은 확률값들은 서로 독립적
  - 따라서 다중 클래스 분류에서 시그모이드 함수는 사용이 불가하며, 대신 소프트맥스 함수를 사용

---

