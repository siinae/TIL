# 웹 크롤링

```python
! pip install selenium
from selenium import webdriver
webdriver.__version__ #버전 확인
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup

service = Service('../chromedriver/chromedriver.exe') 
driver = webdriver.Chrome(sevice = service) #뒤의 service는 위 drive경로가 저장된 service를 의미
url = '크롤링할 페이지 주소 입력'
driver.get(url)

html = driver.page_source

soup = BeautifulSoup(html, 'html.parser') #parser:parsing(구문분석)을 행하는 프로그램
soup 
#페이지의 정보를 전부 다 긁어옴--> 좋지 않은 방식 

#원하는 정보만 가져오기
#페이지의 개발자 도구(f12)에서 어떤 태그에 내가 원하는 정보가 나와있는지 확인


#태그
tags_info = soup.select('span') #span이라는 태그가 있는 정보를 가져옴
len(tags_info) #해당 정보 길이 확인

#id
soup.select('#animals') #id가 animals인 정보를 가져옴 --> id검색시 앞에 '#'을 붙임

#class
soup.select('.name') #class name이 속한 정보를 가져옴
print(soup.select('span.name')) #태그span에서 name이 속한 정보(조건이 많을 경우 쉽게 찾기 위해 사용)  

#여러가지 방법으로 원하는 정보 추출(모두 동일한 결과를 가져옴)
print(soup.select('#animals > span.name'))     #아이디가 fruit1인 정보 중 span.name이 속한 부분
print(soup.select('div.park > #animals > span.name'))  #위와 결과는 같지만 더 정확한 정보 추출이 가능                                                
print(soup.select('div.park > p.animals > span.name'))  #동일한 결과
print(soup.select('div.park span.name'))               #동일한 결과
##크롤링은 이 코드를 수정해나가면서 원하는 정보를 뽑는 것!



#ex. tr태그에 내가 원하는 정보가 있는 경우
info = soup.select('tr') #tr태그에 있는 정보를 뽑아옴
len(info)  #정보의 길이를 통해 내가 원하지 않는 정보가 속해있는지 확인

#ex. tr태그의 header 부분에 내가 원하지 않는 정보가 포함되어 있는 경우

#sol1
info = soup.select('tbody > tr') #tbody태그 아래에 속한 tr태그 정보만 가져올래!
len(info) #다시한번 확인

#sol2
info_list = info[1:]  #info의 첫번째(맨 위) 정보가 필요없음을 확인했으므로 0번째 제거
info_list

#결론--> 같은 정보를 가져오더라도 크롤링의 방식은 다양하다


```

